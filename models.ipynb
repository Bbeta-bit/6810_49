{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7222eead-801a-4ec6-a730-860bf816a63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import numpy as np\n",
    "import scipy \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pygam\n",
    "import sklearn\n",
    "import optuna\n",
    "from sklearn.preprocessing import SplineTransformer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c40b61b-8376-4e12-913e-6a861a90b11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear\n",
    "# Train linear regression model\n",
    "ols = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate\n",
    "ols_pred = ols.predict(X_valid)\n",
    "ols_rmse = np.sqrt(mean_squared_error(y_valid, ols_pred))\n",
    "ols_r2 = r2_score(y_valid, ols_pred)\n",
    "ols_mae = mean_absolute_error(y_valid, ols_pred)\n",
    "\n",
    "print(f\"\\nLinear Regression Performance:\")\n",
    "print(f\"RMSE: {ols_rmse:.2f}\")\n",
    "print(f\"R²: {ols_r2:.4f}\")\n",
    "print(f\"MAE: {ols_mae:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0494d7f-8380-42fa-938f-49565a51c5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear spline\n",
    "\n",
    "def fit_and_plot_spline(feature, n_knots, degree, y_train, train_data):\n",
    "    \"\"\"Fit a spline with specified parameters and plot the results.\"\"\"\n",
    "    # Create the spline transformer\n",
    "    spline = SplineTransformer(n_knots=n_knots, degree=degree, knots='uniform', include_bias=False)\n",
    "    X_spline = spline.fit_transform(train_data[[feature]].values)\n",
    "    \n",
    "    # Fit linear regression on spline features\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_spline, y_train)\n",
    "    \n",
    "    # Create grid for visualization\n",
    "    x_min = train_data[feature].min()\n",
    "    x_max = train_data[feature].max()\n",
    "    x_grid = np.linspace(x_min, x_max, 100).reshape(-1, 1)\n",
    "    X_grid = spline.transform(x_grid)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_grid)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(train_data[feature], y_train, alpha=0.4, label='Data')\n",
    "    plt.plot(x_grid, y_pred, linewidth=2, \n",
    "             label=f'degree {degree} spline with {n_knots} knots')\n",
    "    plt.xlabel(f'{feature} (standardized)')\n",
    "    plt.ylabel('Customer Lifetime Value')\n",
    "    plt.title(f'Regression Spline: Degree {degree} with {n_knots} knots')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1630c4fa-6b27-44eb-8690-4d100a4d6f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Natural Cubic Splines\n",
    "\n",
    "from patsy import dmatrix, build_design_matrices\n",
    "\n",
    "# Create a natural cubic spline with 3 degrees of freedom (df parameter)\n",
    "formula = \"cr(x, df=4) - 1\"  # df=4 gives 3 degrees of freedom after removing the intercept\n",
    "x = train['Ret_Expense']\n",
    "X_natural = dmatrix(formula, {\"x\": x})\n",
    "\n",
    "# Fit model\n",
    "model_natural = LinearRegression()\n",
    "model_natural.fit(X_natural, y_train)\n",
    "\n",
    "# Create a grid for visualization\n",
    "x_grid = np.linspace(x_min, x_max, 100)\n",
    "X_grid_natural = build_design_matrices([X_natural.design_info], {\"x\": x_grid})[0]\n",
    "\n",
    "# Predict\n",
    "y_pred_natural = model_natural.predict(X_grid_natural)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x, y_train, alpha=0.4, label='Data')\n",
    "plt.plot(x_grid, y_pred_natural, linewidth=2, label='Natural Cubic Spline')\n",
    "plt.xlabel('Retention Expense (standardized)')\n",
    "plt.ylabel('Customer Lifetime Value')\n",
    "plt.title('Natural Cubic Spline Regression')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf994397-ebe2-4cf2-a857-904464784059",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GAM\n",
    "from pygam import LinearGAM, s, l\n",
    "\n",
    "# Define the model: s() for smooth terms, l() for linear terms\n",
    "gam = LinearGAM(\n",
    "    s(0) +  # Acq_Expense: smooth term\n",
    "    s(1) +  # Ret_Expense: smooth term \n",
    "    l(2) +  # First_Purchase: linear term\n",
    "    l(3) +  # Revenue: linear term\n",
    "    l(4) +  # Employees: linear term\n",
    "    l(5) +  # Frequency: linear term\n",
    "    l(6) +  # Crossbuy: linear term\n",
    "    l(7)    # Industry: linear term\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "gam.fit(X_train, y_train)\n",
    "\n",
    "# Display model summary\n",
    "print(gam.summary())\n",
    "\n",
    "# Plotting the partial dependence for each feature\n",
    "plt.figure(figsize=(16, 20))\n",
    "\n",
    "# Track the plot position\n",
    "plot_idx = 1\n",
    "\n",
    "for i, term in enumerate(gam.terms):\n",
    "    if term.isintercept:\n",
    "        continue\n",
    "        \n",
    "    # Create subplot (2 plots per row)\n",
    "    plt.subplot(4, 2, plot_idx)\n",
    "    plot_idx += 1\n",
    "    \n",
    "    # Generate points for plotting\n",
    "    X_grid = gam.generate_X_grid(term=i)\n",
    "    \n",
    "    # Get the partial dependence and confidence intervals\n",
    "    pdep, confi = gam.partial_dependence(term=i, X= X_grid, width=0.95)\n",
    "    \n",
    "    # Plot the function\n",
    "    plt.plot(X_grid[:, term.feature], pdep)\n",
    "    plt.plot( X_grid[:, term.feature], confi, c='grey', ls='--', alpha=0.5)\n",
    "    \n",
    "    # Label the plot\n",
    "    plt.title(predictors[i])\n",
    "    plt.grid(True, alpha=0.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "import optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING) # Swithing off ouput, you want to remove this line to see more details\n",
    "\n",
    "# Define a simple objective function to minimize\n",
    "def objective(trial):\n",
    "    # Parameter to optimize\n",
    "    x = trial.suggest_float('x', -10, 10)\n",
    "    \n",
    "    # The function to minimize: (x-2)²\n",
    "    return (x - 2) ** 2\n",
    "\n",
    "# Create a study object and optimize\n",
    "sampler = optuna.samplers.TPESampler(seed=42)  # For reproducibility\n",
    "study = optuna.create_study(direction='minimize', sampler=sampler)\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Print results\n",
    "print(f\"Best value: {study.best_value:.4f}\")\n",
    "print(f\"Best parameters: {study.best_params}\")\n",
    "\n",
    "\n",
    "\n",
    "def gam_objective(trial):\n",
    "    # Separate lambda parameters for different term types\n",
    "    lambda_spline_acq = trial.suggest_float('lambda_spline_acq', 1e-4, 1e4, log=True)\n",
    "    lambda_spline_ret = trial.suggest_float('lambda_spline_ret', 1e-4, 1e4, log=True)\n",
    "    lambda_linear = trial.suggest_float('lambda_linear', 1e-4, 1e4, log=True)\n",
    "    \n",
    "    # Create lambda list with appropriate values for each term\n",
    "    lambdas = [\n",
    "        lambda_spline_acq,  # Acq_Expense (spline)\n",
    "        lambda_spline_ret,  # Ret_Expense (spline)\n",
    "        lambda_linear,   # First_Purchase (linear)\n",
    "        lambda_linear,   # Revenue (linear)\n",
    "        lambda_linear,   # Employees (linear)\n",
    "        lambda_linear,   # Frequency (linear)\n",
    "        lambda_linear,   # Crossbuy (linear)\n",
    "        lambda_linear    # Industry (linear)\n",
    "    ]\n",
    "    \n",
    "    # Create and fit the GAM with these lambda values\n",
    "    model = LinearGAM(s(0) + s(1) + l(2) + l(3) + l(4) + l(5) + l(6) + l(7), \n",
    "                      lam=lambdas)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Return the GCV score\n",
    "    return model.statistics_['GCV']\n",
    "\n",
    "sampler = optuna.samplers.TPESampler(seed=42)  # makes the sampler behave in a deterministic way\n",
    "\n",
    "study = optuna.create_study(direction='minimize', sampler=sampler) # TPE is a method for Bayesian optimisation\n",
    "study.optimize(gam_objective, n_trials=10000, timeout=120) \n",
    "\n",
    "best_lambdas = list(study.best_params.values())\n",
    "\n",
    "lambda_spline_acq = best_lambdas[0]\n",
    "lambda_spline_ret = best_lambdas[1]\n",
    "lambda_linear = best_lambdas[2]\n",
    "    \n",
    "# Create lambda list with appropriate values for each term\n",
    "lambdas = [\n",
    "    lambda_spline_acq,  # Acq_Expense (spline)\n",
    "    lambda_spline_ret,  # Ret_Expense (spline)\n",
    "    lambda_linear,   # First_Purchase (linear)\n",
    "    lambda_linear,   # Revenue (linear)\n",
    "    lambda_linear,   # Employees (linear)\n",
    "    lambda_linear,   # Frequency (linear)\n",
    "    lambda_linear,   # Crossbuy (linear)\n",
    "    lambda_linear    # Industry (linear)\n",
    "]\n",
    "\n",
    "\n",
    "optimized_gam = LinearGAM(s(0) + s(1) + l(2) + l(3) + l(4) + l(5) + l(6) + l(7), lam = lambdas)\n",
    "optimized_gam.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc7f2b2-5e43-43cf-b716-d9f98c0741ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Select most important features\n",
    "subset_indices = [0, 1, 5]  # Income, Limit, Student\n",
    "\n",
    "# Create a column selector using ColumnTransformer\n",
    "column_selector = ColumnTransformer(\n",
    "    [(\"selector\", \"passthrough\", subset_indices)],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# Create the pipeline\n",
    "knn_pipeline = Pipeline([\n",
    "    ('selector', column_selector),\n",
    "    ('knn', KNeighborsRegressor(metric='mahalanobis', \n",
    "                              metric_params={'V': train.iloc[:, subset_indices].cov()}))\n",
    "])\n",
    "\n",
    "# Grid search for optimal number of neighbors\n",
    "param_grid = {'knn__n_neighbors': np.arange(1, 51)}\n",
    "\n",
    "# Perform grid search\n",
    "knn = GridSearchCV(knn_pipeline, param_grid, cv=rkf, scoring='neg_mean_squared_error', verbose=1)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Display best number of neighbours\n",
    "print(f'\\nBest hyperparameters: {knn.best_params_}')\n",
    "\n",
    "# Use the best model\n",
    "knn = knn.best_estimator_\n",
    "\n",
    "# Make predictions and evaluate\n",
    "knn_pred = knn.predict(X_valid)\n",
    "knn_rmse = np.sqrt(mean_squared_error(y_valid, knn_pred))\n",
    "knn_r2 = r2_score(y_valid, knn_pred)\n",
    "knn_mae = mean_absolute_error(y_valid, knn_pred)\n",
    "\n",
    "print(f\"\\nKNN Performance:\")\n",
    "print(f\"RMSE: {knn_rmse:.2f}\")\n",
    "print(f\"R²: {knn_r2:.4f}\")\n",
    "print(f\"MAE: {knn_mae:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316f16f3-10b4-47d5-9d6b-aad0b4c81227",
   "metadata": {},
   "outputs": [],
   "source": [
    "#decision tree\n",
    "model = DecisionTreeClassifier(criterion='entropy', min_samples_leaf=5)\n",
    "\n",
    "search_space = {\n",
    "    'ccp_alpha': alphas,\n",
    "}\n",
    "\n",
    "# Select alpha by grid search\n",
    "tree_search = GridSearchCV(model, search_space, cv = 5 , scoring='neg_log_loss')\n",
    "tree_search.fit(X_train, y_train)\n",
    "tree = tree_search.best_estimator_\n",
    "\n",
    "print('Best parameters found by grid search:', tree_search.best_params_, '\\n')\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "bag = BaggingClassifier(DecisionTreeClassifier(criterion='entropy'), n_estimators=1000, random_state=42)\n",
    "bag.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a211a959-e8e8-4314-a5a9-92cd71010656",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf =  RandomForestClassifier(n_estimators=100,  criterion='entropy',  max_features = 3, min_samples_leaf= 5)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING) # Swithing off ouput, remove this line to see more details\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    criterion = trial.suggest_categorical('criterion', ['entropy', 'gini'])\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 50)\n",
    "    max_features = trial.suggest_int('max_features', 1, 24)\n",
    "    \n",
    "    \n",
    "    model = RandomForestClassifier(n_estimators = 100,  \n",
    "                                  criterion = criterion,  \n",
    "                                  max_features = max_features, \n",
    "                                  min_samples_leaf= min_samples_leaf,\n",
    "                                  random_state = 1)\n",
    "    \n",
    "    scores = cross_val_score(model, X_train, y_train, cv = 5, scoring = 'neg_log_loss')\n",
    "    loss = - np.mean(scores)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "sampler = TPESampler(seed=42) \n",
    "study = optuna.create_study(direction='minimize', sampler=sampler)\n",
    "study.optimize(objective, n_trials = 100, timeout = 120)  # use a higher timeout when trying to maximize accuracy, as opposed to prototyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7984c9f2-288e-4f59-bb99-44e0978e417d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LightGBM (Gradient Boosting)\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "params = {\n",
    " 'objective': 'regression',\n",
    " 'boosting_type': 'gbdt',\n",
    " 'learning_rate': 0.01,\n",
    " 'num_leaves': 45,\n",
    " 'lambda_l1': 0.027109795180157614,\n",
    " 'lambda_l2': 1.3211122996540583e-06,\n",
    " 'bagging_fraction': 0.9303390497096511,\n",
    " 'bagging_freq': 8,\n",
    " 'feature_fraction': 0.6954068017048125,\n",
    " 'min_data_in_leaf': 2}\n",
    "\n",
    "final_lgbm = lgb.train(params, train_data, num_boost_round = 3996)\n",
    "\n",
    "# Make final predictions\n",
    "y_pred_final = final_lgbm.predict(X_valid)\n",
    "\n",
    "# Evaluate final performance\n",
    "rmse_final = np.sqrt(mean_squared_error(y_valid, y_pred_final))\n",
    "r2_final = r2_score(y_valid, y_pred_final)\n",
    "mae_final = mean_absolute_error(y_valid, y_pred_final)\n",
    "\n",
    "print(f\"Final LightGBM Performance:\")\n",
    "print(f\"RMSE: {rmse_final:.4f}\")\n",
    "print(f\"R²: {r2_final:.4f}\")\n",
    "print(f\"MAE: {mae_final:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e784414-6872-4d91-a7cd-93a15970eeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stacking\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "# Create a stacking ensemble with linear regression and KNN as base models\n",
    "stack = StackingRegressor(\n",
    "   estimators=[('Linear Regression', ols), ('KNN', knn)], #other models\n",
    "   final_estimator=LinearRegression(), \n",
    "   cv=5\n",
    ")\n",
    "\n",
    "# Train the stacking ensemble\n",
    "stack.fit(X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
